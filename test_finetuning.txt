"""
Complete implementation of a PyTorch model for CIFAR-10 with ExecutorTorch for mobile deployment 
and on-device fine-tuning capabilities.

This code demonstrates:
1. Building and training a CNN model on CIFAR-10
2. Converting to ExecutorTorch format
3. Implementing on-device fine-tuning
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import numpy as np

# Import ExecutorTorch modules
from executorch.exir import to_edge_transform_and_lower
from torch.export import export, Dim

# Define a simple CNN model for CIFAR-10
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(128 * 4 * 4, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
        
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


def train_model(model, train_loader, val_loader, num_epochs=2, device='cpu'):
    """Train the model and return the trained model."""
    if device == 'cuda' and not torch.cuda.is_available():
        print("CUDA not available, using CPU instead.")
        device = 'cpu'
    
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)
    
    best_val_loss = float('inf')
    best_model_state = None
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item() * inputs.size(0)
        
        epoch_train_loss = running_loss / len(train_loader.dataset)
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        epoch_val_loss = val_loss / len(val_loader.dataset)
        val_accuracy = correct / total
        
        # Update learning rate
        scheduler.step(epoch_val_loss)
        
        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'  Training Loss: {epoch_train_loss:.4f}')
        print(f'  Validation Loss: {epoch_val_loss:.4f}, Accuracy: {val_accuracy:.4f}')
        
        # Save best model
        if epoch_val_loss < best_val_loss:
            best_val_loss = epoch_val_loss
            best_model_state = model.state_dict().copy()
    
    # Load best model state
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    return model


def prepare_cifar10_data(batch_size=64):
    """Prepare CIFAR-10 data loaders for training and testing."""
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    
    # Split training data into train and validation
    indices = list(range(len(train_dataset)))
    np.random.shuffle(indices)
    val_size = int(0.1 * len(train_dataset))
    train_indices = indices[val_size:]
    val_indices = indices[:val_size]
    
    train_subset = Subset(train_dataset, train_indices)
    val_subset = Subset(train_dataset, val_indices)
    
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    
    return train_loader, val_loader, test_loader


def convert_to_executorch(model, example_input, output_dir='./executorch_model'):
    """Convert PyTorch model to ExecutorTorch format."""
    # Ensure the model is in eval mode
    model.eval()
    
    # Create directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Save the PyTorch model
    torch_model_path = os.path.join(output_dir, 'model.pt')
    torch.save(model.state_dict(), torch_model_path)
    
    # Export to ExecutorTorch format using the new API
    print("Exporting model to ExecutorTorch format...")
    
    # For CIFAR-10, we have fixed input sizes (32x32), so no dynamic shapes needed
    # If you want to support variable input sizes, you can uncomment and modify below:
    # dynamic_shapes = {
    #     "x": {
    #         2: Dim("h", min=16, max=64),  # Variable height
    #         3: Dim("w", min=16, max=64),  # Variable width
    #     }
    # }
    
    # Export the model using torch.export (no dynamic shapes for fixed-size CIFAR-10)
    exported_program = export(model, (example_input,))
    
    # Lower to ExecutorTorch using to_edge_transform_and_lower
    executorch_program = to_edge_transform_and_lower(exported_program).to_executorch()
    
    # Save to .pte file
    et_model_path = os.path.join(output_dir, 'model.pte')
    with open(et_model_path, "wb") as f:
        f.write(executorch_program.buffer)
    
    print(f"ExecutorTorch model saved to: {et_model_path}")
    return et_model_path


def prepare_mobile_fine_tuning(model, train_loader, output_dir='./executorch_fine_tuning'):
    """Prepare the model for on-device fine-tuning."""
    os.makedirs(output_dir, exist_ok=True)
    
    # Define a fine-tuning configuration
    fine_tune_config = {
        'learning_rate': 0.0001,
        'batch_size': 8,  # smaller batch size for mobile devices
        'epochs_per_fine_tune': 3,
        'fine_tune_layers': ['classifier.3']  # Only fine-tune the final layer
    }
    
    # Save the configuration
    fine_tune_config_path = os.path.join(output_dir, 'fine_tune_config.pt')
    torch.save(fine_tune_config, fine_tune_config_path)
    
    # Create the exportable fine-tuning module
    class FineTuningModule(nn.Module):
        def __init__(self, base_model):
            super(FineTuningModule, self).__init__()
            self.base_model = base_model
            
            # Create a copy of the tunable parameters only (last layer in this case)
            self.fine_tune_params = nn.ParameterDict()
            for name, param in base_model.named_parameters():
                if any(layer_name in name for layer_name in fine_tune_config['fine_tune_layers']):
                    # Replace dots with underscores to avoid KeyError
                    safe_name = name.replace('.', '_')
                    self.fine_tune_params[safe_name] = nn.Parameter(param.data.clone())
        
        def forward(self, x):
            # Use base model for inference but with the fine-tuned parameters
            with torch.no_grad():
                x = self.base_model.features(x)
                x = torch.flatten(x, 1)
                
                # For the classifier part, we need to use our fine-tuned parameters
                x = self.base_model.classifier[0](x)  # Linear
                x = self.base_model.classifier[1](x)  # ReLU
                x = self.base_model.classifier[2](x)  # Dropout
                
                # Use the fine-tuned classifier.3 parameters (final layer)
                weight = self.fine_tune_params['classifier_3_weight']
                bias = self.fine_tune_params['classifier_3_bias']
                x = torch.nn.functional.linear(x, weight, bias)
            
            return x
    
    # Create and save the fine-tuning module
    fine_tuning_module = FineTuningModule(model)
    fine_tuning_module_path = os.path.join(output_dir, 'fine_tuning_module.pt')
    torch.save(fine_tuning_module.state_dict(), fine_tuning_module_path)
    
    # Trace and export the fine-tuning module for ExecutorTorch
    fine_tuning_module.eval()
    
    # Get a sample batch for tracing
    sample_inputs, sample_labels = next(iter(train_loader))
    sample_input = sample_inputs[0:1]  # Just one example
    
    # Export the fine-tuning inference module
    try:
        # For CIFAR-10 with fixed input size, we don't need dynamic shapes
        # If you want to support variable batch sizes, uncomment below:
        # dynamic_shapes = {
        #     "x": {
        #         0: Dim("batch", min=1, max=16),  # Variable batch size
        #     }
        # }
        
        # Export for inference (no dynamic shapes for fixed-size inputs)
        exported_ft_program = export(fine_tuning_module, (sample_input,))
        executorch_ft_program = to_edge_transform_and_lower(exported_ft_program).to_executorch()
        
        # Save the fine-tuning inference model
        et_inference_path = os.path.join(output_dir, 'fine_tune_inference.pte')
        with open(et_inference_path, "wb") as f:
            f.write(executorch_ft_program.buffer)
        
        print(f"Fine-tuning inference model saved to: {et_inference_path}")
        
    except Exception as e:
        print(f"Warning: Could not export fine-tuning module: {e}")
        print("Fine-tuning will need to be implemented separately on the mobile device.")
    
    print(f"Fine-tuning configuration saved to: {output_dir}")
    return output_dir


def main():
    # Set random seed for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    
    # Set device
    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    device = torch.device('cpu')
    print(f"Using device: {device}")
    
    # Prepare data
    print("Preparing CIFAR-10 data...")
    train_loader, val_loader, test_loader = prepare_cifar10_data(batch_size=64)
    
    # Create and train model
    print("Creating and training model...")
    model = SimpleCNN(num_classes=10)
    trained_model = train_model(model, train_loader, val_loader, num_epochs=2, device=device)
    
    # Evaluate the model
    trained_model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = trained_model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    print(f"Test Accuracy: {accuracy:.2f}%")
    
    # Convert to ExecutorTorch
    print("Converting model to ExecutorTorch format...")
    sample_input = torch.randn(1, 3, 32, 32).to(device)  # Sample input for tracing
    et_model_path = convert_to_executorch(trained_model, sample_input)
    
    # Prepare for mobile fine-tuning
    print("Preparing model for on-device fine-tuning...")
    fine_tuning_dir = prepare_mobile_fine_tuning(trained_model, train_loader)
    
    print("Done! Your model is ready for mobile deployment with on-device fine-tuning capabilities.")


if __name__ == "__main__":
    main()
